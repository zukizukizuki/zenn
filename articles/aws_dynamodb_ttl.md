---
title: "ã€AWSã€‘DynamoDBã§TTLã«é”ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’S3 Glacierã¸ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã™ã‚‹æ§‹æˆ"
emoji: "ğŸ·"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: [AWS , DynamoDB, terraform , Python, lambda]
published: true
---

## æ¦‚è¦

ã“ã®æ§‹æˆã¯ã€IoTãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰é€ä¿¡ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’DynamoDBã«æ ¼ç´ã—ã€è¨­å®šã—ãŸTTLï¼ˆTime To Liveï¼‰ã«åŸºã¥ã„ã¦è‡ªå‹•çš„ã«å‰Šé™¤ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ã€ä½ã‚³ã‚¹ãƒˆãªS3 Glacierã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã™ã‚‹ã‚‚ã®ã§ã™ã€‚

## ç›®çš„

*   **ãƒ‡ãƒ¼ã‚¿ä¿æŒè¦ä»¶ã®éµå®ˆ:** ä¸€å®šæœŸé–“çµŒéã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‚‚ã®ã®ã€ç›£æŸ»ã‚„åˆ†æã®ãŸã‚ã«é•·æœŸçš„ãªä¿ç®¡ãŒå¿…è¦ãªå ´åˆã«ã€ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚ˆããƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã—ã¾ã™ã€‚
*   **ã‚³ã‚¹ãƒˆæœ€é©åŒ–:** ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯é«˜é€ŸãªDynamoDBã«ã€ã‚¢ã‚¯ã‚»ã‚¹é »åº¦ã®ä½ã„éå»ãƒ‡ãƒ¼ã‚¿ã¯ä½ã‚³ã‚¹ãƒˆãªS3 Glacierã«ä¿ç®¡ã™ã‚‹ã“ã¨ã§ã€å…¨ä½“çš„ãªã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã¾ã™ã€‚
*   **é‹ç”¨è² è·ã®è»½æ¸›:** TTLã«ã‚ˆã‚‹è‡ªå‹•å‰Šé™¤ã¨ã€å‰Šé™¤ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«ã‚ˆã‚Šã€æ‰‹å‹•ã§ã®ãƒ‡ãƒ¼ã‚¿ç®¡ç†ä½œæ¥­ã‚’å‰Šæ¸›ã—ã¾ã™ã€‚

## æ§‹æˆè¦ç´ 

1.  **IoT Core (SQS):** IoTãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€ã¾ãšSQS (ã¾ãŸã¯IoT Coreãƒ«ãƒ¼ãƒ«ã‚¨ãƒ³ã‚¸ãƒ³) ã‚’çµŒç”±ã—ã¦Lambdaé–¢æ•°ã«æ¸¡ã•ã‚Œã¾ã™ã€‚

2.  **Lambda (SQS -> DynamoDB):** SQSã«æ ¼ç´ã•ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€DynamoDBã«æ›¸ãè¾¼ã‚€Lambdaé–¢æ•°ã§ã™ã€‚ã“ã®Lambdaé–¢æ•°ã¯ã€ãƒ‡ãƒ¼ã‚¿ã«TTLã‚’è¨­å®šã™ã‚‹ãŸã‚ã®`expiredAt`å±æ€§ã‚’è¿½åŠ ã—ã¾ã™ã€‚

3.  **DynamoDB:** å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹NoSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã™ã€‚`expiredAt`å±æ€§ã‚’TTLå±æ€§ã¨ã—ã¦è¨­å®šã™ã‚‹ã“ã¨ã§ã€æŒ‡å®šã•ã‚ŒãŸæœŸé–“ãŒçµŒéã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•çš„ã«å‰Šé™¤ã—ã¾ã™ã€‚

4.  **DynamoDB Streams:** DynamoDBãƒ†ãƒ¼ãƒ–ãƒ«ã®å¤‰æ›´å±¥æ­´ã‚’è¨˜éŒ²ã™ã‚‹æ©Ÿèƒ½ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã€æ›´æ–°ã€å‰Šé™¤ãªã©ã®ã‚¤ãƒ™ãƒ³ãƒˆãŒç™ºç”Ÿã™ã‚‹ã¨ã€ãã®å†…å®¹ãŒã‚¹ãƒˆãƒªãƒ¼ãƒ ã«è¨˜éŒ²ã•ã‚Œã¾ã™ã€‚

5.  **Lambda (DynamoDB Streams -> S3 Glacier):** DynamoDB Streamsã‹ã‚‰ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’å—ã‘å–ã‚Šã€TTLã«ã‚ˆã£ã¦å‰Šé™¤ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§S3 Glacierã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã™ã‚‹Lambdaé–¢æ•°ã§ã™ã€‚

6.  **S3 Glacier:** é•·æœŸçš„ãªãƒ‡ãƒ¼ã‚¿ä¿ç®¡ã«é©ã—ãŸã€ä½ã‚³ã‚¹ãƒˆãªã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯ã€å¿…è¦ãªå ´åˆã«å–å¾—ã—ã¦åˆ†æãªã©ã«åˆ©ç”¨ã§ãã¾ã™ã€‚

## æ§‹æˆå›³

![](https://storage.googleapis.com/zenn-user-upload/017dba26ed62-20250218.png)

## ã‚³ãƒ¼ãƒ‰

### SQS(IoT Core)ã‹ã‚‰JSONã‚’å¸ã„ä¸Šã’ã¦DynamoDBã«å…¥ã‚Œã‚‹Lambdaã®terraformè¨­å®š
```
################
# lambdaã®ã‚½ãƒ¼ã‚¹ã®zip
################
data "archive_file" "function_archive" {
  type        = "zip"
  source_dir  = "${path.module}/lambda_putSensorData/"
  output_path = "${path.module}/lambda_putSensorData/output/functions.zip"
}

############################
# Iotãƒ‡ãƒ¼ã‚¿ã‚’lambdaã«ã‚­ãƒ¥ãƒ¼ã™ã‚‹ãŸã‚ã®SQS
############################
resource "aws_sqs_queue" "ba_sqs" {
  name                       = "${var.environment}_ba_sqs"
  visibility_timeout_seconds = 900
  tags                       = { environment = "${var.environment}" }

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Principal = {
          AWS = "arn:aws:iam::<ACCOUNT_ID>:role/<IOT_ROLE_NAME>" # ç§˜åŒ¿æƒ…å ±: AWSã‚¢ã‚«ã‚¦ãƒ³ãƒˆIDã¨IAMãƒ­ãƒ¼ãƒ«åã‚’éš è”½
        }
        Action   = "sqs:SendMessage"
        Resource = "arn:aws:sqs:ap-northeast-1:<AWS_ACCOUNT_ID>:${var.environment}_ba_sqs" # ç§˜åŒ¿æƒ…å ±: AWSã‚¢ã‚«ã‚¦ãƒ³ãƒˆIDã‚’éš è”½
      }
    ]
  })
}

######################
# S3ã«é…ç½®ã™ã‚‹lambdaã‚½ãƒ¼ã‚¹ã®zip
######################
resource "aws_s3_object" "lambda_zip" {
  bucket = aws_s3_bucket.lambda_bucket.bucket
  key    = "cross-account-${filemd5(data.archive_file.function_archive.output_path)}.zip"
  source = data.archive_file.function_archive.output_path
  etag   = filemd5(data.archive_file.function_archive.output_path)
  tags   = { environment = "${var.environment}" }
}


########################
# ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã®lambdaé–¢æ•°
########################
resource "aws_lambda_function" "sensor_data_ingest_lambda" {
  architectures    = ["arm64"]
  function_name    = "${var.environment}-sensor-data-ingest"
  handler          = "lambda_function.lambda_handler"
  memory_size      = 128
  package_type     = "Zip"
  role             = data.aws_iam_role.admin_for_lambda.arn # é©åˆ‡ãªIAMãƒ­ãƒ¼ãƒ«ã®ARNã‚’è¨­å®š
  runtime          = "python3.12"
  s3_bucket        = aws_s3_bucket.lambda_bucket.bucket
  s3_key           = aws_s3_object.lambda_zip.key
  source_code_hash = data.archive_file.function_archive.output_base64sha256
  tags = {
    "lambda:createdBy" = "SAM",
    environment        = "${var.environment}"
  }
  tags_all = {
    "lambda:createdBy" = "SAM"
  }
  timeout = 900
  environment {
    variables = merge(var.lambda_environment_variables, {
      DEVICE_DATA = "${var.environment}_DeviceData"
    })
  }
  logging_config {
    log_format = "Text"
    log_group  = "/aws/lambda/${var.environment}-sensor-data-ingest" # ãƒ­ã‚°ã‚°ãƒ«ãƒ¼ãƒ—åã‚‚å¤‰æ›´
  }
  tracing_config {
    mode = "PassThrough"
  }
}

####################
# lambdaã®ãƒˆãƒªã‚¬ãƒ¼å¯¾è±¡ã‚’æŒ‡å®š
####################
resource "aws_lambda_event_source_mapping" "sqs_source_mapping" {
  event_source_arn                   = aws_sqs_queue.ba_sqs.arn
  function_name                      = aws_lambda_function.sensor_data_ingest_lambda.arn
  batch_size                         = 20
  maximum_batching_window_in_seconds = 60
  bisect_batch_on_function_error     = false
  enabled                            = true
  tags                               = { environment = "${var.environment}" }
  lifecycle {
    ignore_changes = [enabled]
  }
}
```

### SQS(IoT Core)ã‹ã‚‰JSONã‚’å¸ã„ä¸Šã’ã¦expiredAt(TTL)ã‚’è¿½åŠ ã—ãŸä¸Šã§DynamoDBã«å…¥ã‚Œã‚‹Lambda

```python
import boto3
import os
import json
import base64
from decimal import Decimal
import logging
import time

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger()
logger.setLevel(logging.WARNING)  # WARNINGãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®ãƒ­ã‚°å‡ºåŠ›

dynamoDB = boto3.resource('dynamodb')

# 1å¹´åˆ†ã®ç§’æ•°
ONE_YEAR_IN_SECONDS = 31536000

def lambda_handler(event, context):
    table = dynamoDB.Table(os.environ['DEVICE_DATA'])

    with table.batch_writer(overwrite_by_pkeys=['deviceID', 'createdAt']) as batch:
        for record in event['Records']:
            message_body = record['body']

            # Base64 ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã®å‡¦ç†
            try:
                decoded_bytes = base64.b64decode(message_body)
                decoded_str = decoded_bytes.decode('utf-8')
            except (base64.binascii.Error, UnicodeDecodeError) as e:
                logger.error(f'Error decoding or parsing message body: {e}')
                continue

            try:
                data = json.loads(decoded_str, parse_float=Decimal)
            except json.JSONDecodeError as e:
                logger.error(f'Error parsing JSON: {e}')
                continue
            
            if "things_name" in data:
              client_id = data.get('things_name', '')
              data['topic'] = client_id + "/SENSOR/DAT"  #topicã‚’è¿½åŠ 
              data['client_id'] = client_id #client_idã‚’è¿½åŠ 

            logger.info(f'Received data: {data}')  # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°

            if is_except_data(data.get('client_id', '')):
                data = {k.lower(): v for k, v in data.items()}
                data = convert_floats_to_decimals(data)
                try:
                    put_device_data(data, batch) #expiredAtã‚’æ¸¡ã™å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“
                except KeyError as e:
                    logger.error(f'KeyError: {e}. Data: {data}')
                    continue

    return {
        'statusCode': 200
    }

def is_except_data(things_name):
    if os.environ.get('APP_ENV', '') == "Staging":
        return 'stg' in things_name
    else:
        return 'stg' not in things_name

def put_device_data(data, batch):
    
    # ãƒ‡ãƒ¼ã‚¿å†…ã«dataã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã™ã‚‹
    data_value = data.get('data', None)
    
    if data_value:  # dataã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
        if isinstance(data_value, dict): # dataãŒè¾æ›¸å‹ã®å ´åˆ
           
            # 'createdAt' ã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ 'timestamp' ã®å€¤ã‚’ä½¿ç”¨
            createdAt = data_value.get('createdAt', data.get('timestamp'))
            if createdAt is None:
                createdAt = data.get('timestamp')
                if createdAt is None:
                  logger.error(f'KeyError: createdAt and timestamp are both None. Data: {data}')
                  return

            if len(str(createdAt)) == 10:
                createdAt *= 1000
            # expiredAt ã‚’è¨ˆç®—
            expiredAt = int(createdAt / 1000) + ONE_YEAR_IN_SECONDS  # ãƒŸãƒªç§’å˜ä½ã‹ã‚‰ç§’å˜ä½ã«å¤‰æ›ã—ã¦ã‹ã‚‰1å¹´ã‚’è¶³ã™
            data_value['expiredAt'] = expiredAt #data_value ã« expiredAt ã‚’è¿½åŠ 

            data_value.update({
                'createdAt': createdAt,
                'deviceID': data.get('topic', '').split('/')[0],
                'sensorID': data.get('topic', '').split('/')[0] + "-sen",
            })

            logger.info(f'Putting item into DynamoDB: {data_value}') # INFOãƒ¬ãƒ™ãƒ«ãªã®ã§å‡ºåŠ›ã•ã‚Œã¾ã™
            try:
                batch.put_item(Item=data_value)
            except Exception as e:
                logger.error(f'Error putting item into DynamoDB: {e}. Item: {data_value}')


        elif isinstance(data_value, list): # dataãŒãƒªã‚¹ãƒˆå‹ã®å ´åˆ
            for item in data_value:
                if item is None:
                    logger.error(f'Item is None. Data: {data}')
                    continue
                
                # item ãŒè¾æ›¸å‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                if isinstance(item, dict):
                    # 'createdAt' ã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ 'timestamp' ã®å€¤ã‚’ä½¿ç”¨
                    createdAt = item.get('createdAt', data.get('timestamp'))
                    if createdAt is None:
                        logger.error(f'KeyError: createdAt and timestamp are both None. Data: {data}')
                        continue  # ã¾ãŸã¯ raise KeyError('createdAt and timestamp') ã§ã‚¨ãƒ©ãƒ¼ã‚’æŠ•ã’ã‚‹

                    if len(str(createdAt)) == 10:
                        createdAt *= 1000

                    # expiredAt ã‚’è¨ˆç®—
                    expiredAt = int(createdAt / 1000) + ONE_YEAR_IN_SECONDS  # ãƒŸãƒªç§’å˜ä½ã‹ã‚‰ç§’å˜ä½ã«å¤‰æ›ã—ã¦ã‹ã‚‰1å¹´ã‚’è¶³ã™
                    item['expiredAt'] = expiredAt #item ã« expiredAt ã‚’è¿½åŠ 

                    item.update({
                        'createdAt': createdAt,
                        'deviceID': data.get('topic', '').split('/')[0],
                        'sensorID': data.get('topic', '').split('/')[0] + "-sen",
                    })

                    logger.info(f'Putting item into DynamoDB: {item}') # INFOãƒ¬ãƒ™ãƒ«ãªã®ã§å‡ºåŠ›ã•ã‚Œã¾ã™
                    try:
                        batch.put_item(Item=item)
                    except Exception as e:
                        logger.error(f'Error putting item into DynamoDB: {e}. Item: {item}')
                else:
                   logger.error(f'Item is not a dict. Item: {item}. Data: {data}')
        else: # data ãŒè¾æ›¸ã§ã‚‚ãƒªã‚¹ãƒˆã§ã‚‚ãªã„å ´åˆ
            logger.error(f'Data is not a dict or a list. Data: {data}')
    else: # dataã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆã€dataã‚’å˜ä¸€ã‚¢ã‚¤ãƒ†ãƒ ã¨ã—ã¦å‡¦ç†ã™ã‚‹
        # 'createdAt' ã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ 'timestamp' ã®å€¤ã‚’ä½¿ç”¨
        createdAt = data.get('createdat', data.get('timestamp'))
        if createdAt is None:
            logger.error(f'KeyError: createdAt and timestamp are both None. Data: {data}')
            return  # ã¾ãŸã¯ raise KeyError('createdAt and timestamp') ã§ã‚¨ãƒ©ãƒ¼ã‚’æŠ•ã’ã‚‹

        if len(str(createdAt)) == 10:
            createdAt *= 1000
        
        if "things_name" in data:
          client_id = data.get('things_name', '')
        else:
          client_id = data.get('topic', '').split('/')[0]

        # expiredAt ã‚’è¨ˆç®—
        expiredAt = int(createdAt / 1000) + ONE_YEAR_IN_SECONDS  # ãƒŸãƒªç§’å˜ä½ã‹ã‚‰ç§’å˜ä½ã«å¤‰æ›ã—ã¦ã‹ã‚‰1å¹´ã‚’è¶³ã™
        data['expiredAt'] = expiredAt #dataã«expiredAtã‚’è¿½åŠ 
        data.update({
            'createdAt': createdAt,
            'deviceID': client_id,
            'sensorID': client_id + "-sen",
        })
        logger.info(f'Putting item into DynamoDB: {data}') # INFOãƒ¬ãƒ™ãƒ«ãªã®ã§å‡ºåŠ›ã•ã‚Œã¾ã™

        try:
            batch.put_item(Item=data)
        except Exception as e:
            logger.error(f'Error putting item into DynamoDB: {e}. Item: {data}')

def convert_floats_to_decimals(obj):
    if isinstance(obj, float):
        return Decimal(str(obj))
    elif isinstance(obj, dict):
        for key, value in obj.items():
            obj[key] = convert_floats_to_decimals(value)
    elif isinstance(obj, list):
        obj = [convert_floats_to_decimals(item) for item in obj]
    return obj
```

### DynamoDBã®Terraformè¨­å®š

```terraform
resource "aws_dynamodb_table" "DeviceData" {
  billing_mode                = "PROVISIONED"
  deletion_protection_enabled = false
  hash_key                    = "deviceID"
  name                        = "${var.environment}_DeviceData"
  range_key                   = "createdAt"
  table_class                 = "STANDARD"
  stream_enabled              = true
  stream_view_type            = "OLD_IMAGE"
  tags                        = { environment = "${var.environment}" }

  attribute {
    name = "createdAt"
    type = "N"
  }
  attribute {
    name = "deviceID"
    type = "S"
  }

  point_in_time_recovery {
    enabled = false
  }

  read_capacity  = 38 # hazaviewã®æ¨å¥¨èª­ã¿è¾¼ã¿ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£
  write_capacity = 22 # hazaviewã®æ¨å¥¨æ›¸ãè¾¼ã¿ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£

  ttl {
    attribute_name = "expiredAt"
    enabled        = true
  }
}
```

### TTLã‚’è¿ãˆãŸãƒ‡ãƒ¼ã‚¿ã‚’S3 Glacierã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã™ã‚‹Lambda

```python
import boto3
import json
import os
import logging

# S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
s3 = boto3.client('s3')

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰S3ãƒã‚±ãƒƒãƒˆåã‚’å–å¾—
S3_BUCKET = os.environ.get('S3_BUCKET')

# ãƒ­ã‚¬ãƒ¼ã‚’åˆæœŸåŒ–
logger = logging.getLogger()
logger.setLevel(logging.WARNING)  # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’WARNINGã«è¨­å®š (INFOã‹ã‚‰å¤‰æ›´)

def lambda_handler(event, context):
    """
    Lambdaé–¢æ•°ã®ãƒãƒ³ãƒ‰ãƒ©

    DynamoDBã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†ã—ã€TTLã‚’è¿ãˆã¦å‰Šé™¤ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’S3 Glacierã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã™ã‚‹ã€‚
    REMOVEã‚¤ãƒ™ãƒ³ãƒˆã«é–¢ã™ã‚‹ãƒ­ã‚°ã®ã¿ã‚’å‡ºåŠ›ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´ã€‚

    Args:
        event (dict): Lambdaã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ (DynamoDBã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¤ãƒ™ãƒ³ãƒˆ)
        context (LambdaContext): Lambdaã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

    Returns:
        dict: å‡¦ç†çµæœã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """

    # S3_BUCKETç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if not S3_BUCKET:
        logger.error("S3_BUCKET environment variable is not set.")
        return {
            'statusCode': 500,
            'body': json.dumps({'message': 'S3_BUCKET environment variable is not set.'})
        }

    logger.info("Received event: %s", json.dumps(event)) # ã‚¤ãƒ™ãƒ³ãƒˆå…¨ä½“ã®ãƒ­ã‚°ã¯INFOãƒ¬ãƒ™ãƒ«ã§å‡ºåŠ› (ãƒ‡ãƒãƒƒã‚°ç”¨ã¨ã—ã¦æ®‹ã™)

    for record in event['Records']:
        if record['eventName'] == 'REMOVE': # REMOVEã‚¤ãƒ™ãƒ³ãƒˆ (TTLå‰Šé™¤) ã®å ´åˆã®ã¿å‡¦ç†
            logger.info("Processing REMOVE record: %s", json.dumps(record)) # REMOVEã‚¤ãƒ™ãƒ³ãƒˆã®å‡¦ç†é–‹å§‹ãƒ­ã‚°ã‚’INFOãƒ¬ãƒ™ãƒ«ã§å‡ºåŠ›
            if 'dynamodb' in record and 'OldImage' in record['dynamodb']: # OldImageãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                dynamodb_data = record['dynamodb']['OldImage']
                try:
                    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’æŒ‡å®šã—ã¦å€¤ã‚’å–å¾—
                    device_id = dynamodb_data['deviceID']['S']
                    created_at = dynamodb_data['createdAt']['N']
                    #expired_at = dynamodb_data['expiredAt']['N']  # è¿½åŠ : expiredAt ã®å€¤ã‚’å–å¾—

                    # S3ã«ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å (ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–/deviceID/createdAt.json)
                    s3_key = f"archive/{device_id}/{created_at}.json"

                    # DynamoDBã®ãƒ‡ãƒ¼ã‚¿ã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ› (æ—¥ä»˜å‹ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹ãŸã‚ã®defaultæŒ‡å®š)
                    json_data = json.dumps(dynamodb_data, default=str)

                    # S3 Glacier ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                    s3.put_object(
                        Bucket=S3_BUCKET,
                        Key=s3_key,
                        Body=json_data,
                        StorageClass='GLACIER'
                    )
                    logger.info(f"Archived data to s3://{S3_BUCKET}/{s3_key} with storage class GLACIER")

                except KeyError as e:
                    logger.exception(f"KeyError processing record: {e}.  Check if 'deviceID' and 'createdAt' exist and have the correct type (S and N).") # ã‚¨ãƒ©ãƒ¼è©³ç´°ãƒ­ã‚° (ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’å«ã‚€) ã‚’ERRORãƒ¬ãƒ™ãƒ«ã§å‡ºåŠ›
                    return {
                        'statusCode': 500,
                        'body': json.dumps({'message': f'Error archiving data to S3: {e}'})
                    }

                except Exception as e:
                    logger.exception(f"Error processing record: {e}") # ã‚¨ãƒ©ãƒ¼è©³ç´°ãƒ­ã‚° (ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’å«ã‚€) ã‚’ERRORãƒ¬ãƒ™ãƒ«ã§å‡ºåŠ›
                    return {
                        'statusCode': 500,
                        'body': json.dumps({'message': f'Error archiving data to S3: {e}'})
                    }
            else:
                logger.warning("OldImage not found in REMOVE event. Skipping archive for this record.") # è­¦å‘Šãƒ­ã‚°ã‚’WARNINGãƒ¬ãƒ™ãƒ«ã§å‡ºåŠ› (OldImageãŒãªã„å ´åˆ)
        # REMOVEã‚¤ãƒ™ãƒ³ãƒˆä»¥å¤–ã®å ´åˆã¯ãƒ­ã‚°å‡ºåŠ›ã—ãªã„ (INFOãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®šã®ãŸã‚)

    return {
        'statusCode': 200,
        'body': json.dumps({'message': 'Successfully processed DynamoDB records.'})
    }
```

### S3ãƒã‚±ãƒƒãƒˆã¨Lambdaé–¢æ•°ã®è¨­å®š

```terraform
resource "aws_s3_bucket" "archive_bucket" {
  bucket = "${var.environment}-dynamodb-archive-bucket"
}

data "archive_file" "lambda_zip" {
  type        = "zip"
  source_dir  = "${path.module}/lambda_DynamoDB_archive/"
  output_path = "${path.module}/lambda_DynamoDB_archive/output/functions.zip"
}

resource "aws_lambda_function" "archive_lambda" {
  function_name = "${var.environment}-dynamodb-archive"
  description   = "Lambda function to archive DynamoDB TTL deleted items to S3 Glacier"
  role          = data.aws_iam_role.for_lambda.arn
  handler       = "lambda_function.lambda_handler"
  runtime       = "python3.9"
  timeout       = 300
  memory_size   = 128
  filename      = data.archive_file.lambda_zip.output_path

  environment {
    variables = {
      S3_BUCKET = aws_s3_bucket.archive_bucket.bucket
    }
  }
  source_code_hash = data.archive_file.lambda_zip.output_base64sha256
}

# DynamoDB ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒˆãƒªã‚¬ãƒ¼ã®ä½œæˆ (Lambda é–¢æ•°ã‚’ãƒˆãƒªã‚¬ãƒ¼)
resource "aws_lambda_event_source_mapping" "dynamodb_trigger" {
  event_source_arn  = aws_dynamodb_table.DeviceData.stream_arn
  function_name     = aws_lambda_function.archive_lambda.arn
  starting_position = "LATEST"
  batch_size        = 100
}
```

## è£œè¶³
*   `${var.environment}`ã¯ç’°å¢ƒå¤‰æ•°ã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚
*   `data.aws_iam_role.for_lambda.arn`ã¯ã€é©åˆ‡ãªIAMãƒ­ãƒ¼ãƒ«ã®ARNã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚
*   ä¸Šè¨˜ã¯ä¾‹ã§ã‚ã‚Šã€å¿…è¦ã«å¿œã˜ã¦è¨­å®šã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚
